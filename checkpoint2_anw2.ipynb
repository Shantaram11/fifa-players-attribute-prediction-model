{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a48ee332-3f56-4dac-a96c-327f1027d133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-3.5.2-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40e0b11-23e6-4560-980a-8c776f0d8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaec13e-e213-45a7-83ab-d7bd99e4f4de",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5a40a98-fe03-4f35-a336-c1094bab95e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.5.2-bin-hadoop3\\python\\pyspark\\sql\\context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "appName = \"Big Data Analytics\"\n",
    "master = \"local\"\n",
    "\n",
    "conf = pyspark.SparkConf()\\\n",
    ".set('spark.driver.host','127.0.0.1')\\\n",
    ".setAppName(appName)\\\n",
    ".setMaster(master)\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef08445d-0dad-4229-886a-51460dc4d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, monotonically_increasing_id\n",
    "df_list = list()\n",
    "\n",
    "## read in all data for male players\n",
    "for i in range(15, 23):\n",
    "    file_name = \"data\\\\players_\" + str(i) + \".csv\"\n",
    "    df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "    year = 2000 + i\n",
    "    gender = \"Male\"\n",
    "    df = df.withColumn(\"year\", lit(year))\n",
    "    df = df.withColumn(\"gender\", lit(gender))\n",
    "    df_list.append(df)\n",
    "    \n",
    "## read in all data for female players\n",
    "for i in range(16, 23):\n",
    "    file_name = \"data\\\\female_players_\" + str(i) + \".csv\"\n",
    "    df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "    year = 2000 + i\n",
    "    gender = \"Female\"\n",
    "    df = df.withColumn(\"year\", lit(year))\n",
    "    df = df.withColumn(\"gender\", lit(gender))\n",
    "    df_list.append(df)\n",
    "    \n",
    "## merge all data into one dataframe\n",
    "df_merged = df_list[0]\n",
    "for df in df_list[1:]:\n",
    "    df_merged = df_merged.union(df)\n",
    "    \n",
    "## create new column to storage unique id for each piece of data \n",
    "df_merged = df_merged.withColumn(\"record_id\", monotonically_increasing_id())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46b7cf62-1e62-463d-9ffc-5048c70693ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## write data into PostgreDB\n",
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"QQwa43420024420-\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "table = \"fifa.players\"\n",
    "df_merged.write \\\n",
    "    .jdbc(url=url, table=table, mode=\"overwrite\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a39a81-c236-421c-980a-de4dced35e23",
   "metadata": {},
   "source": [
    "code to create schema fifa and table fifa.players refer to task1.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74aa981e-7d6e-4818-a6a7-673ebb12e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data from PostgreDB\n",
    "db_properties={}\n",
    "db_properties['username'] = \"postgres\"\n",
    "db_properties['password'] = \"QQwa43420024420-\"\n",
    "db_properties['url'] = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "db_properties['driver'] = \"org.postgresql.Driver\"\n",
    "db_properties['table'] = \"fifa.players\"\n",
    "\n",
    "df_read = sqlContext.read.format(\"jdbc\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".load()\n",
    "\n",
    "## Only analyzing data for male players\n",
    "df_read = df_read.filter(df_read.gender == \"Male\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0636620f-f021-4517-a3ca-73d20103bcf4",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f97bc4-5027-44c7-a47d-15689a17fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc, asc\n",
    "\n",
    "## Task 2.1\n",
    "def get_clubs_with_most_players(year, n_club, ending_year):\n",
    "    df_casted = df_read.withColumn(\"club_contract_valid_until_int\", col(\"club_contract_valid_until\").cast(\"integer\"))\n",
    "    df_filtered = df_casted.filter((df_casted.year == year) & (df_casted.club_contract_valid_until_int >= ending_year))\n",
    "    df_grouped = df_filtered.groupBy(\"club_name\").count()\n",
    "    df_ordered = df_grouped.orderBy(desc(\"count\"))\n",
    "    df_limited = df_ordered.limit(n_club)\n",
    "    print(f\"the {n_club} clubs with most players in year {year} whose contract ending in or after year {ending_year}\")\n",
    "    df_limited.show()\n",
    "\n",
    "## Task 2.2 parameter order is in [\"highest\", \"lowest\"]\n",
    "def get_clubs_with_highest_or_lowest_average_age(year, n_club, order):\n",
    "    original_n = n_club\n",
    "    df_filtered = df_read.filter(df_read.year == year)\n",
    "    df_grouped = df_filtered.groupBy(\"club_name\").avg(\"age\")\n",
    "    if order == \"lowest\":\n",
    "        df_ordered = df_grouped.orderBy(asc(\"avg(age)\"))\n",
    "    elif order == \"highest\":\n",
    "        df_ordered = df_grouped.orderBy(desc(\"avg(age)\"))\n",
    "    if n_club < df_ordered.count():\n",
    "        while 1:\n",
    "            if df_ordered.limit(n_club).collect()[-1][\"avg(age)\"] == df_ordered.limit(n_club+1).collect()[-1][\"avg(age)\"]:\n",
    "                n_club += 1\n",
    "            else:\n",
    "                break\n",
    "    df_limited = df_ordered.limit(n_club)\n",
    "    print(f\"the {original_n} clubs with {order} average ages for players in year {year}\")\n",
    "    df_limited.show()\n",
    "\n",
    "## Task 2.3\n",
    "def get_most_popular_nationality(year):\n",
    "    df_filtered = df_read.filter(df_read.year == year)\n",
    "    df_grouped = df_filtered.groupBy(\"nationality_name\").count()\n",
    "    df_ordered = df_grouped.orderBy(desc(\"count\"))\n",
    "    df_limited = df_ordered.limit(1)\n",
    "    print(f\"Most Popular Nationality in Year {year}\")\n",
    "    df_limited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e4d4be3-4b24-47a6-b19e-f57655b973bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 clubs with most players in year 2020 whose contract ending in or after year 2024\n",
      "+-------------------+-----+\n",
      "|          club_name|count|\n",
      "+-------------------+-----+\n",
      "|   Deportes Iquique|   12|\n",
      "|Patriotas Boyacá FC|   12|\n",
      "|          Al Ain FC|   11|\n",
      "|  Alianza Petrolera|   11|\n",
      "|     Atlético Huila|   11|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Test task 2.1 get_clubs_with_most_players(year, n_club, ending_year)\n",
    "get_clubs_with_most_players(2020, 5, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1ccdba-d8f3-4ca2-9715-82a60287fa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 3 clubs with highest average ages for players in year 2020\n",
      "+--------------------+--------+\n",
      "|           club_name|avg(age)|\n",
      "+--------------------+--------+\n",
      "|           Fortaleza|    32.6|\n",
      "|            Cruzeiro|    31.6|\n",
      "|Club Athletico Pa...|    31.4|\n",
      "|            Botafogo|    31.4|\n",
      "|Associação Chapec...|    31.4|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Test task 2.2 get_clubs_with_highest_or_lowest_average_age(year, n_club, order)\n",
    "get_clubs_with_highest_or_lowest_average_age(2020, 3, \"highest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01196db3-2dd6-4eac-9d35-b06fd92b3a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Popular Nationality in Year 2015\n",
      "+----------------+-----+\n",
      "|nationality_name|count|\n",
      "+----------------+-----+\n",
      "|         England| 1627|\n",
      "+----------------+-----+\n",
      "\n",
      "Most Popular Nationality in Year 2016\n",
      "+----------------+-----+\n",
      "|nationality_name|count|\n",
      "+----------------+-----+\n",
      "|         England| 1519|\n",
      "+----------------+-----+\n",
      "\n",
      "Most Popular Nationality in Year 2017\n",
      "+----------------+-----+\n",
      "|nationality_name|count|\n",
      "+----------------+-----+\n",
      "|         England| 1627|\n",
      "+----------------+-----+\n",
      "\n",
      "Most Popular Nationality in Year 2018\n",
      "+----------------+-----+\n",
      "|nationality_name|count|\n",
      "+----------------+-----+\n",
      "|         England| 1633|\n",
      "+----------------+-----+\n",
      "\n",
      "Most Popular Nationality in Year 2019\n",
      "+----------------+-----+\n",
      "|nationality_name|count|\n",
      "+----------------+-----+\n",
      "|         England| 1625|\n",
      "+----------------+-----+\n",
      "\n",
      "Most Popular Nationality in Year 2020\n",
      "+----------------+-----+\n",
      "|nationality_name|count|\n",
      "+----------------+-----+\n",
      "|         England| 1670|\n",
      "+----------------+-----+\n",
      "\n",
      "Most Popular Nationality in Year 2021\n",
      "+----------------+-----+\n",
      "|nationality_name|count|\n",
      "+----------------+-----+\n",
      "|         England| 1685|\n",
      "+----------------+-----+\n",
      "\n",
      "Most Popular Nationality in Year 2022\n",
      "+----------------+-----+\n",
      "|nationality_name|count|\n",
      "+----------------+-----+\n",
      "|         England| 1719|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Test task 2.3 get_most_popular_nationality(year)\n",
    "for year in range(2015, 2023):\n",
    "    get_most_popular_nationality(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c03f5-53ba-4952-84fb-d204cb7d8a20",
   "metadata": {},
   "source": [
    "Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "365886ab-8d51-4672-8ab4-c76ed9b6e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocession\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# define all useful features\n",
    "feature_cols_int = [\"weak_foot\", \"skill_moves\", \"international_reputation\", \"pace\", \"shooting\", \"passing\", \"dribbling\", \"defending\", \"physic\", \"attacking_crossing\", \"attacking_finishing\", \"attacking_heading_accuracy\", \"attacking_short_passing\", \"attacking_volleys\", \"skill_dribbling\", \"skill_curve\", \"skill_fk_accuracy\", \"skill_long_passing\", \"skill_ball_control\", \"movement_acceleration\", \"movement_sprint_speed\", \"movement_agility\", \"movement_reactions\", \"movement_balance\", \"power_shot_power\", \"power_jumping\", \"power_stamina\", \"power_strength\", \"power_long_shots\", \"mentality_aggression\", \"mentality_interceptions\", \"mentality_positioning\", \"mentality_vision\", \"mentality_penalties\", \"defending_marking_awareness\", \"defending_standing_tackle\", \"defending_sliding_tackle\", \"goalkeeping_diving\", \"goalkeeping_handling\", \"goalkeeping_kicking\", \"goalkeeping_positioning\", \"goalkeeping_reflexes\", \"goalkeeping_speed\"]\n",
    "feature_cols_to_onehot = [\"work_rate\", \"mentality_composure\", \"ls\", \"st\", \"rs\", \"lw\", \"lf\", \"cf\", \"rf\", \"rw\", \"lam\", \"cam\", \"ram\", \"lm\", \"lcm\", \"cm\", \"rcm\", \"rm\", \"lwb\", \"ldm\", \"cdm\", \"rdm\", \"rwb\", \"lb\", \"lcb\", \"cb\", \"rcb\", \"rb\", \"gk\"]\n",
    "outcome_col = [\"overall\"]\n",
    "df = df_read.select(feature_cols_int + feature_cols_to_onehot + outcome_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d296adf4-f936-4250-a125-0a42128ecaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------\n",
      " weak_foot                   | 0.0                \n",
      " skill_moves                 | 0.0                \n",
      " international_reputation    | 0.0                \n",
      " pace                        | 11.114239261256062 \n",
      " shooting                    | 11.114239261256062 \n",
      " passing                     | 11.114239261256062 \n",
      " dribbling                   | 11.114239261256062 \n",
      " defending                   | 11.114239261256062 \n",
      " physic                      | 11.114239261256062 \n",
      " attacking_crossing          | 0.0                \n",
      " attacking_finishing         | 0.0                \n",
      " attacking_heading_accuracy  | 0.0                \n",
      " attacking_short_passing     | 0.0                \n",
      " attacking_volleys           | 0.0                \n",
      " skill_dribbling             | 0.0                \n",
      " skill_curve                 | 0.0                \n",
      " skill_fk_accuracy           | 0.0                \n",
      " skill_long_passing          | 0.0                \n",
      " skill_ball_control          | 0.0                \n",
      " movement_acceleration       | 0.0                \n",
      " movement_sprint_speed       | 0.0                \n",
      " movement_agility            | 0.0                \n",
      " movement_reactions          | 0.0                \n",
      " movement_balance            | 0.0                \n",
      " power_shot_power            | 0.0                \n",
      " power_jumping               | 0.0                \n",
      " power_stamina               | 0.0                \n",
      " power_strength              | 0.0                \n",
      " power_long_shots            | 0.0                \n",
      " mentality_aggression        | 0.0                \n",
      " mentality_interceptions     | 0.0                \n",
      " mentality_positioning       | 0.0                \n",
      " mentality_vision            | 0.0                \n",
      " mentality_penalties         | 0.0                \n",
      " defending_marking_awareness | 0.0                \n",
      " defending_standing_tackle   | 0.0                \n",
      " defending_sliding_tackle    | 0.0                \n",
      " goalkeeping_diving          | 0.0                \n",
      " goalkeeping_handling        | 0.0                \n",
      " goalkeeping_kicking         | 0.0                \n",
      " goalkeeping_positioning     | 0.0                \n",
      " goalkeeping_reflexes        | 0.0                \n",
      " goalkeeping_speed           | 88.88576073874394  \n",
      " work_rate                   | 0.0                \n",
      " mentality_composure         | 22.366429943904446 \n",
      " ls                          | 0.0                \n",
      " st                          | 0.0                \n",
      " rs                          | 0.0                \n",
      " lw                          | 0.0                \n",
      " lf                          | 0.0                \n",
      " cf                          | 0.0                \n",
      " rf                          | 0.0                \n",
      " rw                          | 0.0                \n",
      " lam                         | 0.0                \n",
      " cam                         | 0.0                \n",
      " ram                         | 0.0                \n",
      " lm                          | 0.0                \n",
      " lcm                         | 0.0                \n",
      " cm                          | 0.0                \n",
      " rcm                         | 0.0                \n",
      " rm                          | 0.0                \n",
      " lwb                         | 0.0                \n",
      " ldm                         | 0.0                \n",
      " cdm                         | 0.0                \n",
      " rdm                         | 0.0                \n",
      " rwb                         | 0.0                \n",
      " lb                          | 0.0                \n",
      " lcb                         | 0.0                \n",
      " cb                          | 0.0                \n",
      " rcb                         | 0.0                \n",
      " rb                          | 0.0                \n",
      " gk                          | 0.0                \n",
      " overall                     | 0.0                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate the ratio of null value for each feature\n",
    "df_count = df.count()\n",
    "null_counts_df = df.select([(count(when(isnan(col(c)) | col(c).isNull(), c))/df_count*100).alias(c) \\\n",
    "                        for c in df.columns])\n",
    "null_counts_df.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f1c604c-2874-4e83-a70b-f9108fa8e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"goalkeeping_speed\" because tremendous missing values, then drop the rest data if there is missing value in each row\n",
    "df = df.drop(\"goalkeeping_speed\")\n",
    "feature_cols_int.remove(\"goalkeeping_speed\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "741a8ae7-85f6-4d41-a264-34a343d677e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode all features and generate final df\n",
    "for c in (feature_cols_int + outcome_col):\n",
    "    df = df.withColumn(c+\"_encoded\", col(c).cast(DoubleType()))\n",
    "feature_cols_int_to_double = [x+\"_encoded\" for x in (feature_cols_int + outcome_col)]\n",
    "feature_cols_index = [x+\"_index\" for x in feature_cols_to_onehot]\n",
    "feature_cols_onehot = [x+\"_encoded\" for x in feature_cols_to_onehot]\n",
    "indexer = StringIndexer(inputCols=feature_cols_to_onehot, outputCols=feature_cols_index, handleInvalid=\"keep\")\n",
    "df_index_encoded = indexer.fit(df).transform(df)\n",
    "encoder = OneHotEncoder(inputCols=feature_cols_index, outputCols=feature_cols_onehot, handleInvalid=\"keep\")\n",
    "df_onehot_encoded = encoder.fit(df_index_encoded).transform(df_index_encoded)\n",
    "assembler = VectorAssembler(inputCols=feature_cols_int_to_double + feature_cols_onehot, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "df_assembled = assembler.transform(df_onehot_encoded)\n",
    "df_final = df_assembled.select([\"features\", \"overall_encoded\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "596b4dfb-7136-4b46-b3c9-dc94b6d7eb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------\n",
      " features        | (7993,[0,1,2,3,4,... \n",
      " overall_encoded | 66.0                 \n",
      "-RECORD 1-------------------------------\n",
      " features        | (7993,[0,1,2,3,4,... \n",
      " overall_encoded | 94.0                 \n",
      "-RECORD 2-------------------------------\n",
      " features        | (7993,[0,1,2,3,4,... \n",
      " overall_encoded | 93.0                 \n",
      "-RECORD 3-------------------------------\n",
      " features        | (7993,[0,1,2,3,4,... \n",
      " overall_encoded | 92.0                 \n",
      "-RECORD 4-------------------------------\n",
      " features        | (7993,[0,1,2,3,4,... \n",
      " overall_encoded | 92.0                 \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example cases of df_final\n",
    "df_final.show(5, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "070ddf7d-8ace-443c-a4f7-15b7670b140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 50 distinct labels\n"
     ]
    }
   ],
   "source": [
    "# there are only 50 different labels, so that the problem can be regarded as classification problem\n",
    "N_labels = df_final.select(\"overall_encoded\").distinct().count()\n",
    "print(f\"there are {N_labels} distinct labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c5bb2e8-7dc4-4d1a-becd-f341ce6c9940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only randomly choose 10000 rows as the dataset for further training and testing to save time\n",
    "df_omit = df_final.sample(withReplacement=False, fraction=1.0, seed=2024).limit(10000)\n",
    "train_data, test_data = df_omit.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fb87a7d-546e-4b68-bc95-0f8a1fbe53b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when regParam=0.01, maxIter=50, there is highest accuracy for Logistic Regression: 79.62037962037962%\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression on spark\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "regParam_range = [0.01, 0.1]\n",
    "maxIter_range = [10, 30, 50]\n",
    "tuning_list = []\n",
    "for regParam in regParam_range:\n",
    "    for maxIter in maxIter_range:\n",
    "        lr = LogisticRegression(featuresCol=\"features\", labelCol=\"overall_encoded\", regParam=regParam, maxIter=maxIter)\n",
    "        lr_model = lr.fit(train_data)\n",
    "        predictions = lr_model.transform(test_data)\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"overall_encoded\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        accuracy = evaluator.evaluate(predictions)\n",
    "        tuning_list.append((regParam, maxIter, accuracy))\n",
    "tuning_list = sorted(tuning_list, key=lambda x:x[-1])\n",
    "best_tuple = tuning_list[-1]\n",
    "best_regParam = best_tuple[0]\n",
    "best_maxIter = best_tuple[1]\n",
    "best_accuracy = best_tuple[2]\n",
    "print(f\"when regParam={best_regParam}, maxIter={best_maxIter}, there is highest accuracy for Logistic Regression: {100*best_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d6c5749-e503-41d7-bd98-220e46ecc057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when impurity=entropy, maxDepth=10, there is highest accuracy for Decision Tree: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Decision Classifier on Spark\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "maxDepth_range = [10, 30]\n",
    "impurity_range = [\"gini\", \"entropy\"]\n",
    "\n",
    "tuning_list = []\n",
    "for impurity in impurity_range:\n",
    "    for maxDepth in maxDepth_range:\n",
    "        dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"overall_encoded\", maxDepth=maxDepth, impurity=impurity)\n",
    "        dt_model = dt.fit(train_data)\n",
    "        predictions = dt_model.transform(test_data)\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"overall_encoded\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        accuracy = evaluator.evaluate(predictions)\n",
    "        tuning_list.append((impurity, maxDepth, accuracy))\n",
    "tuning_list = sorted(tuning_list, key=lambda x:x[-1])\n",
    "best_tuple = tuning_list[-1]\n",
    "best_impurity = best_tuple[0]\n",
    "best_maxDepth = best_tuple[1]\n",
    "best_accuracy = best_tuple[2]\n",
    "print(f\"when impurity={best_impurity}, maxDepth={best_maxDepth}, there is highest accuracy for Decision Tree: {100*best_accuracy}%\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f39775f-0873-4c44-a504-cb0c9d8ad993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data from spark to tensor\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "train_data_pd = train_data.toPandas()\n",
    "test_data_pd = test_data.toPandas()\n",
    "X_train = torch.Tensor(train_data_pd[\"features\"])\n",
    "y_train = torch.Tensor(train_data_pd[\"overall_encoded\"])\n",
    "X_test = torch.Tensor(test_data_pd[\"features\"])\n",
    "y_test = torch.Tensor(test_data_pd[\"overall_encoded\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22ed7e9f-ae2d-4dfb-bc04-16d7acdbb5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch neural network 1 without hidden layer\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "N_labels = df_final.select(\"overall_encoded\").distinct().count()\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx],self.y[idx])\n",
    "\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "test_dataset = MyDataset(X_test, y_test)\n",
    "\n",
    "class myModel1(nn.Module):\n",
    "    def __init__(self, input_dims, output_dims):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(input_dims, output_dims),\n",
    "        )\n",
    "    def forward(self, X):\n",
    "            return self.seq(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb87c07b-5fdb-4dfb-a109-e8778d3630b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myModel1(\n",
      "  (seq): Sequential(\n",
      "    (0): Linear(in_features=7993, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model1 structure\n",
    "model = myModel1(train_dataset.x.shape[1], 1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9eb38cfb-7321-4e09-a198-d28be774eb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when lr=0.1, batch_size=500, there is highest accuracy for Model1: 41.60839080810547%\n"
     ]
    }
   ],
   "source": [
    "lr_range = [0.005, 0.05, 0.1]\n",
    "batch_size_range = [500, 2000]\n",
    "N_epochs = 50\n",
    "tuning_list = []\n",
    "for lr in lr_range:\n",
    "    for batch_size in batch_size_range:\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle =True)\n",
    "        model = myModel1(train_dataset.x.shape[1], 1)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        for epoch in range(N_epochs):\n",
    "            for batch_id, (x_batch, y_batch) in enumerate(train_dataloader):\n",
    "                predictions = model(x_batch)\n",
    "                loss = loss_fn(predictions, y_batch.reshape(-1, 1))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        pred = model(X_test)\n",
    "        accuracy = 100 * torch.sum(torch.round(pred) == y_test.reshape(-1, 1)) / len(y_test)\n",
    "        tuning_list.append((model, lr, batch_size, accuracy))\n",
    "\n",
    "tuning_list = sorted(tuning_list, key=lambda x:x[-1])\n",
    "best_tuple = tuning_list[-1]\n",
    "best_lr = best_tuple[1]\n",
    "best_batch_size = best_tuple[2]\n",
    "best_accuracy = best_tuple[3]\n",
    "print(f\"when lr={best_lr}, batch_size={best_batch_size}, there is highest accuracy for Model1: {best_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d964513-55e0-4e05-a043-e8e80020949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch neural network 2 with 3 hidden layers, each layer with 128 neurons\n",
    "class myModel2(nn.Module):\n",
    "    def __init__(self, input_dims, output_dims):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(input_dims, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dims)\n",
    "            \n",
    "        )\n",
    "    def forward(self, X):\n",
    "            return self.seq(X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "149d989c-d019-4342-91d0-c7699bf4f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myModel2(\n",
      "  (seq): Sequential(\n",
      "    (0): Linear(in_features=7993, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model2 structure\n",
    "model = myModel2(train_dataset.x.shape[1], 1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd51e4cb-8c42-49a5-8621-aa1bf7545b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when lr=0.005, batch_size=500, there is highest accuracy: 33.61638259887695%\n"
     ]
    }
   ],
   "source": [
    "lr_range = [0.005, 0.05, 0.1]\n",
    "batch_size_range = [500, 2000]\n",
    "N_epochs = 50\n",
    "tuning_list = []\n",
    "for lr in lr_range:\n",
    "    for batch_size in batch_size_range:\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle =True)\n",
    "        model = myModel2(train_dataset.x.shape[1], 1)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        for epoch in range(N_epochs):\n",
    "            for batch_id, (x_batch, y_batch) in enumerate(train_dataloader):\n",
    "                predictions = model(x_batch)\n",
    "                loss = loss_fn(predictions, y_batch.reshape(-1, 1))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        pred = model(X_test)\n",
    "        accuracy = 100 * torch.sum(pred.int() == y_test.reshape(-1, 1).int()) / len(y_test)\n",
    "        tuning_list.append((model, lr, batch_size, accuracy))\n",
    "\n",
    "tuning_list = sorted(tuning_list, key=lambda x:x[-1])\n",
    "best_tuple = tuning_list[-1]\n",
    "best_lr = best_tuple[1]\n",
    "best_batch_size = best_tuple[2]\n",
    "best_accuracy = best_tuple[3]\n",
    "print(f\"when lr={best_lr}, batch_size={best_batch_size}, there is highest accuracy: {best_accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f75976e-18a1-4114-b9ef-ff72449017ea",
   "metadata": {},
   "source": [
    "Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb1f96-0c74-406e-a449-615788952240",
   "metadata": {},
   "source": [
    "refer to Task4.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
